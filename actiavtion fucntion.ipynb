{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7515bd2-176a-43da-ab35-583a79d7967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. \n",
    "# We know, the neural network has neurons that work in correspondence with weight, bias, and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fded58-1840-4464-908d-a98d2ee1448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 We need to use activation functions such as ReLu, sigmoid, and tanh to give the neural network a non-linear property. This way, the network can model more complex relationships and patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f77e08e-3ef9-46ab-9bc5-4ee03f14f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3Activation functions determine how neurons transform input signals into output activations during forward propagation. During backpropagation, gradients calculated for each layer depend on the derivative of the activation function.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723edefb-a2e6-4c5b-a603-51964a920000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4This function takes any real value as input and outputs values in the range of 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941dbd14-ca0a-434e-9b9f-6174ab4a145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "# advantages \n",
    "#The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "#disadvantages \n",
    "# Vanishing gradient problem. Sigmoids saturate and kill gradients. The output isn't zero centered thus the gradient updates go too far in different directions i.e. 0 < output < 1, and it makes optimization harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec1ebf4-19e7-42ec-941f-c91951099d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 ReLU boasts of training complex models/neural networks with constant values compared to sigmoid. Relu exhibits better efficiency: An advantage of ReLU, despite avoiding the vanishing gradients problem, is that it has a much lower run time max(0,f) and runs much faster than any sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b8a367-05fd-4001-92ad-64fa0900b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 ReLU boasts of training complex models/neural networks with constant values compared to sigmoid. Relu exhibits better efficiency: An advantage of ReLU, despite avoiding the vanishing gradients problem, is that it has a much lower run time max(0,f) and runs much faster than any sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818b4dad-d292-4694-a547-49ee41746933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7The Leaky ReLU is a popular activation function that is used to address the limitations of the standard ReLU function in deep neural networks by introducing a small negative slope for negative function inputs, which helps neural networks to maintain better information flow both during its training and after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e25b0ad-6e61-4f32-ad4a-6839662f8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 The problem with the use of ReLU is when the gradient has a value of 0. In such cases, the node is considered as a dead node since the old and new values of the weights remain the same. This situation can be avoided by the use of a leaky ReLU function which prevents the gradient from falling to the zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861659bd-ca39-4b5f-a219-0f44d65be275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
